У теорії ймовірності та статистиці, коваріа́ція (англ. covariance) — це міра спільної мінливості двох випадкових змінних. Якщо більші значення однієї змінної здебільшого відповідають більшим значенням іншої, й те саме виконується для менших значень, тобто змінні схильні демонструвати подібну поведінку, то коваріація є додатною. В протилежному випадку, коли більші значення однієї змінної здебільшого відповідають меншим значенням іншої, тобто змінні схильні демонструвати протилежну поведінку, коваріація є від'ємною. Отже, знак коваріації показує тенденцію в лінійному взаємозв'язку між цими змінними. Величину ж коваріації інтерпретувати непросто. Проте унормована версія коваріації, коефіцієнт кореляції, показує своєю величиною силу цього лінійного взаємозв'язку.
Слід розрізняти (1) коваріацію двох випадкових змінних, яка є параметром сукупності, що можна розглядати як властивість спільного розподілу ймовірності, та (2) вибіркову коваріацію, яка на додачу до того, що вона слугує описом вибірки, слугує також і оцінкою значення параметру сукупності.


== Визначення ==
Коваріацію між двома спільно розподіленими дійснозначними випадковими змінними X та Y зі скінченними другими моментами визначають як математичне сподівання добутку їхніх відхилень від їхніх особистих математичних сподівань:

  
    
      
        cov
        ⁡
        (
        X
        ,
        Y
        )
        =
        E
        ⁡
        
          
            
              [
            
          
          (
          X
          −
          E
          ⁡
          [
          X
          ]
          )
          (
          Y
          −
          E
          ⁡
          [
          Y
          ]
          )
          
            
              ]
            
          
        
        ,
      
    
    {\displaystyle \operatorname {cov} (X,Y)=\operatorname {E} {{\big [}(X-\operatorname {E} [X])(Y-\operatorname {E} [Y]){\big ]}},}
  де E[X] є математичним сподіванням X, відомим також як середнє значення X. Коваріацію також іноді позначують через «σ», за аналогією з дисперсією. Використовуючи властивість лінійності математичних сподівань, це можна спростити до математичного сподівання їхнього добутку мінус добуток їхніх математичних сподівань:

  
    
      
        
          
            
              
                cov
                ⁡
                (
                X
                ,
                Y
                )
              
              
                
                =
                E
                ⁡
                
                  [
                  
                    
                      (
                      
                        X
                        −
                        E
                        ⁡
                        
                          [
                          X
                          ]
                        
                      
                      )
                    
                    
                      (
                      
                        Y
                        −
                        E
                        ⁡
                        
                          [
                          Y
                          ]
                        
                      
                      )
                    
                  
                  ]
                
              
            
            
              
              
                
                =
                E
                ⁡
                
                  [
                  
                    X
                    Y
                    −
                    X
                    E
                    ⁡
                    
                      [
                      Y
                      ]
                    
                    −
                    E
                    ⁡
                    
                      [
                      X
                      ]
                    
                    Y
                    +
                    E
                    ⁡
                    
                      [
                      X
                      ]
                    
                    E
                    ⁡
                    
                      [
                      Y
                      ]
                    
                  
                  ]
                
              
            
            
              
              
                
                =
                E
                ⁡
                
                  [
                  
                    X
                    Y
                  
                  ]
                
                −
                E
                ⁡
                
                  [
                  X
                  ]
                
                E
                ⁡
                
                  [
                  Y
                  ]
                
                −
                E
                ⁡
                
                  [
                  X
                  ]
                
                E
                ⁡
                
                  [
                  Y
                  ]
                
                +
                E
                ⁡
                
                  [
                  X
                  ]
                
                E
                ⁡
                
                  [
                  Y
                  ]
                
              
            
            
              
              
                
                =
                E
                ⁡
                
                  [
                  
                    X
                    Y
                  
                  ]
                
                −
                E
                ⁡
                
                  [
                  X
                  ]
                
                E
                ⁡
                
                  [
                  Y
                  ]
                
                .
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}\operatorname {cov} (X,Y)&=\operatorname {E} \left[\left(X-\operatorname {E} \left[X\right]\right)\left(Y-\operatorname {E} \left[Y\right]\right)\right]\\&=\operatorname {E} \left[XY-X\operatorname {E} \left[Y\right]-\operatorname {E} \left[X\right]Y+\operatorname {E} \left[X\right]\operatorname {E} \left[Y\right]\right]\\&=\operatorname {E} \left[XY\right]-\operatorname {E} \left[X\right]\operatorname {E} \left[Y\right]-\operatorname {E} \left[X\right]\operatorname {E} \left[Y\right]+\operatorname {E} \left[X\right]\operatorname {E} \left[Y\right]\\&=\operatorname {E} \left[XY\right]-\operatorname {E} \left[X\right]\operatorname {E} \left[Y\right].\end{aligned}}}
  Проте коли 
  
    
      
        E
        ⁡
        [
        X
        Y
        ]
        ≈
        E
        ⁡
        [
        X
        ]
        E
        ⁡
        [
        Y
        ]
      
    
    {\displaystyle \operatorname {E} [XY]\approx \operatorname {E} [X]\operatorname {E} [Y]}
  , це крайнє рівняння схильне до катастрофічного анулювання, якщо його обчислюють за допомогою арифметики з рухомою комою, і відтак його слід уникати в комп'ютерних програмах, якщо дані не було попередньо відцентровано. В такому разі слід віддавати перевагу чисельно стійким алгоритмам.
Для випадкових векторів 
  
    
      
        
          X
        
        ∈
        
          
            R
          
          
            m
          
        
      
    
    {\displaystyle \mathbf {X} \in \mathbb {R} ^{m}}
   та 
  
    
      
        
          Y
        
        ∈
        
          
            R
          
          
            n
          
        
      
    
    {\displaystyle \mathbf {Y} \in \mathbb {R} ^{n}}
   взаємно-коваріаційна матриця m × n (відома також як дисперсі́йна ма́триця, англ. dispersion matrix, або дисперсі́йно-коваріаці́йна ма́триця, англ. variance–covariance matrix, або просто коваріаційна матриця) дорівнює

  
    
      
        
          
            
              
                cov
                ⁡
                (
                
                  X
                
                ,
                
                  Y
                
                )
              
              
                
                =
                E
                ⁡
                
                  [
                  
                    (
                    
                      X
                    
                    −
                    E
                    ⁡
                    [
                    
                      X
                    
                    ]
                    )
                    (
                    
                      Y
                    
                    −
                    E
                    ⁡
                    [
                    
                      Y
                    
                    ]
                    
                      )
                      
                        
                          T
                        
                      
                    
                  
                  ]
                
              
            
            
              
              
                
                =
                E
                ⁡
                
                  [
                  
                    
                      X
                    
                    
                      
                        Y
                      
                      
                        
                          T
                        
                      
                    
                  
                  ]
                
                −
                E
                ⁡
                [
                
                  X
                
                ]
                E
                ⁡
                [
                
                  Y
                
                
                  ]
                  
                    
                      T
                    
                  
                
                ,
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}\operatorname {cov} (\mathbf {X} ,\mathbf {Y} )&=\operatorname {E} \left[(\mathbf {X} -\operatorname {E} [\mathbf {X} ])(\mathbf {Y} -\operatorname {E} [\mathbf {Y} ])^{\mathrm {T} }\right]\\&=\operatorname {E} \left[\mathbf {X} \mathbf {Y} ^{\mathrm {T} }\right]-\operatorname {E} [\mathbf {X} ]\operatorname {E} [\mathbf {Y} ]^{\mathrm {T} },\end{aligned}}}
  де mT є транспонуванням вектору (або матриці) m.
(i, j)-тий елемент цієї матриці дорівнює коваріації cov(Xi, Yj) між i-тою скалярною складовою X та j-тою скалярною складовою Y. Зокрема, cov(Y, X) є транспонуванням cov(X, Y).
Для вектору 
  
    
      
        
          X
        
        =
        
          
            
              [
              
                
                  
                    
                      X
                      
                        1
                      
                    
                  
                  
                    
                      X
                      
                        2
                      
                    
                  
                  
                    …
                  
                  
                    
                      X
                      
                        m
                      
                    
                  
                
              
              ]
            
          
          
            
              T
            
          
        
      
    
    {\displaystyle \mathbf {X} ={\begin{bmatrix}X_{1}&X_{2}&\dots &X_{m}\end{bmatrix}}^{\mathrm {T} }}
   з m спільно розподілених випадкових змінних зі скінченними другими моментами, його коваріаційну матрицю визначають як

  
    
      
        Σ
        (
        
          X
        
        )
        =
        cov
        ⁡
        (
        
          X
        
        ,
        
          X
        
        )
        .
      
    
    {\displaystyle \Sigma (\mathbf {X} )=\operatorname {cov} (\mathbf {X} ,\mathbf {X} ).}
  Випадкові змінні, чия коваріація є нульовою, називають некорельованими. Аналогічно, випадкові вектори, чия коваріаційна матриця є нульовою в усіх елементах за межами головної діагоналі, називають некорельованими.
Одиницями вимірювання коваріації cov(X, Y) є добуток одиниць X та Y. На противагу цьому, коефіцієнти кореляції, які залежать від коваріації, є безрозмірнісною мірою лінійної залежності. (Насправді, коефіцієнти кореляції можна розуміти як просто унормовану версію коваріації.)


=== Дискретні змінні ===
Якщо кожна зі змінних має скінченний набір рівноймовірних значень, 
  
    
      
        
          x
          
            i
          
        
      
    
    {\displaystyle x_{i}}
   та 
  
    
      
        
          y
          
            j
          
        
      
    
    {\displaystyle y_{j}}
   відповідно для 
  
    
      
        i
        =
        1
        ,
        …
        ,
        n
      
    
    {\displaystyle i=1,\dots ,n}
   та 
  
    
      
        j
        =
        1
        ,
        …
        ,
        k
      
    
    {\displaystyle j=1,\dots ,k}
  , то коваріацію може бути рівнозначно записано в термінах середніх значень 
  
    
      
        E
        (
        X
        )
      
    
    {\displaystyle E(X)}
   та 
  
    
      
        E
        (
        Y
        )
      
    
    {\displaystyle E(Y)}
   як

  
    
      
        cov
        ⁡
        (
        X
        ,
        Y
        )
        =
        
          
            1
            
              n
              k
            
          
        
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          ∑
          
            j
            =
            1
          
          
            k
          
        
        (
        
          x
          
            i
          
        
        −
        E
        (
        X
        )
        )
        (
        
          y
          
            j
          
        
        −
        E
        (
        Y
        )
        )
        .
      
    
    {\displaystyle \operatorname {cov} (X,Y)={\frac {1}{nk}}\sum _{i=1}^{n}\sum _{j=1}^{k}(x_{i}-E(X))(y_{j}-E(Y)).}
  Якщо 
  
    
      
        n
        =
        k
      
    
    {\displaystyle n=k}
  , то її також може бути рівнозначно виражено без прямого посилання на середні як

  
    
      
        cov
        ⁡
        (
        X
        ,
        Y
        )
        =
        
          
            1
            
              n
              
                2
              
            
          
        
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          ∑
          
            j
            =
            1
          
          
            n
          
        
        
          
            1
            2
          
        
        (
        
          x
          
            i
          
        
        −
        
          x
          
            j
          
        
        )
        ⋅
        (
        
          y
          
            i
          
        
        −
        
          y
          
            j
          
        
        )
        =
        
          
            1
            
              n
              
                2
              
            
          
        
        
          ∑
          
            i
          
        
        
          ∑
          
            j
            >
            i
          
        
        (
        
          x
          
            i
          
        
        −
        
          x
          
            j
          
        
        )
        ⋅
        (
        
          y
          
            i
          
        
        −
        
          y
          
            j
          
        
        )
        .
      
    
    {\displaystyle \operatorname {cov} (X,Y)={\frac {1}{n^{2}}}\sum _{i=1}^{n}\sum _{j=1}^{n}{\frac {1}{2}}(x_{i}-x_{j})\cdot (y_{i}-y_{j})={\frac {1}{n^{2}}}\sum _{i}\sum _{j>i}(x_{i}-x_{j})\cdot (y_{i}-y_{j}).}
  Більш загальний випадок, якщо роглядаємо 
  
    
      
        n
      
    
    {\displaystyle n}
   можливих реалізацій 
  
    
      
        (
        X
        ,
        Y
        )
      
    
    {\displaystyle (X,Y)}
  , позачені 
  
    
      
        (
        
          x
          
            i
          
        
        ,
        
          y
          
            i
          
        
        )
      
    
    {\displaystyle (x_{i},y_{i})}
   але з ймовірністю 
  
    
      
        
          p
          
            i
          
        
      
    
    {\displaystyle p_{i}}
   для 
  
    
      
        i
        =
        1
        ,
        …
        ,
        n
      
    
    {\displaystyle i=1,\ldots ,n}
  , то коваріяція дорівнює

  
    
      
        cov
        ⁡
        (
        X
        ,
        Y
        )
        =
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          p
          
            i
          
        
        ⋅
        (
        
          x
          
            i
          
        
        −
        E
        (
        X
        )
        )
        ⋅
        (
        
          y
          
            i
          
        
        −
        E
        (
        Y
        )
        )
        .
      
    
    {\displaystyle \operatorname {cov} (X,Y)=\sum _{i=1}^{n}p_{i}\cdot (x_{i}-E(X))\cdot (y_{i}-E(Y)).}
  Приклад для дискретних випадкових зміннихПрипустімо, що X та Y мають наступну спільну функцію маси ймовірності:

Тоді 
  
    
      
        
          μ
          
            X
          
        
        =
        
          
            3
            2
          
        
      
    
    {\displaystyle \mu _{X}={\frac {3}{2}}}
  , 
  
    
      
        
          μ
          
            Y
          
        
        =
        2
      
    
    {\displaystyle \mu _{Y}=2}
  , 
  
    
      
        
          σ
          
            X
          
        
        =
        
          
            1
            2
          
        
      
    
    {\displaystyle \sigma _{X}={\frac {1}{2}}}
  , а 
  
    
      
        
          σ
          
            Y
          
        
        =
        
          
            
              1
              2
            
          
        
        .
      
    
    {\displaystyle \sigma _{Y}={\sqrt {\frac {1}{2}}}.}
  

  
    
      
        
          
            
              
              
                cov
                ⁡
                (
                X
                ,
                Y
                )
                =
                
                  σ
                  
                    X
                    Y
                  
                
                =
                
                  ∑
                  
                    (
                    x
                    ,
                    y
                    )
                    ∈
                    S
                  
                
                (
                x
                −
                
                  μ
                  
                    X
                  
                
                )
                (
                y
                −
                
                  μ
                  
                    Y
                  
                
                )
                f
                (
                x
                ,
                y
                )
              
            
            
              
                =
                

                
              
              
                
                  (
                  
                    1
                    −
                    
                      
                        3
                        2
                      
                    
                  
                  )
                
                (
                1
                −
                2
                )
                
                  (
                  
                    
                      1
                      4
                    
                  
                  )
                
                +
                
                  (
                  
                    1
                    −
                    
                      
                        3
                        2
                      
                    
                  
                  )
                
                (
                2
                −
                2
                )
                
                  (
                  
                    
                      1
                      4
                    
                  
                  )
                
              
            
            
              
              
                
                

                
                +
                
                  (
                  
                    1
                    −
                    
                      
                        3
                        2
                      
                    
                  
                  )
                
                (
                3
                −
                2
                )
                (
                0
                )
                +
                
                  (
                  
                    2
                    −
                    
                      
                        3
                        2
                      
                    
                  
                  )
                
                (
                1
                −
                2
                )
                (
                0
                )
              
            
            
              
              
                
                

                
                +
                
                  (
                  
                    2
                    −
                    
                      
                        3
                        2
                      
                    
                  
                  )
                
                (
                2
                −
                2
                )
                
                  (
                  
                    
                      1
                      4
                    
                  
                  )
                
                +
                
                  (
                  
                    2
                    −
                    
                      
                        3
                        2
                      
                    
                  
                  )
                
                (
                3
                −
                2
                )
                
                  (
                  
                    
                      1
                      4
                    
                  
                  )
                
              
            
            
              
                =
                

                
              
              
                
                  
                    1
                    4
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}&\operatorname {cov} (X,Y)=\sigma _{XY}=\sum _{(x,y)\in S}(x-\mu _{X})(y-\mu _{Y})f(x,y)\\={}&\left(1-{\frac {3}{2}}\right)(1-2)\left({\frac {1}{4}}\right)+\left(1-{\frac {3}{2}}\right)(2-2)\left({\frac {1}{4}}\right)\\&{}+\left(1-{\frac {3}{2}}\right)(3-2)(0)+\left(2-{\frac {3}{2}}\right)(1-2)(0)\\&{}+\left(2-{\frac {3}{2}}\right)(2-2)\left({\frac {1}{4}}\right)+\left(2-{\frac {3}{2}}\right)(3-2)\left({\frac {1}{4}}\right)\\={}&{\frac {1}{4}}\end{aligned}}}
  Додаткові приклади можна знайти тут.


== Властивості ==
Дисперсія (англ. variance) є окремим випадком коваріації, в якому обидві змінні є ідентичними (тобто в якому одна зі змінних завжди набуває такого ж значення, як і інша):
  
    
      
        cov
        ⁡
        (
        X
        ,
        X
        )
        =
        var
        ⁡
        (
        X
        )
        ≡
        
          σ
          
            2
          
        
        (
        X
        )
        ≡
        
          σ
          
            X
          
          
            2
          
        
        .
      
    
    {\displaystyle \operatorname {cov} (X,X)=\operatorname {var} (X)\equiv \sigma ^{2}(X)\equiv \sigma _{X}^{2}.}
  Якщо X, Y, W та V є дійснозначними випадковими змінними, а a, b, c, d є сталими («стала» в цьому контексті означає не випадкова), то наступні факти є наслідком визначення коваріації:
  
    
      
        
          
            
              
                σ
                (
                X
                ,
                a
                )
              
              
                
                =
                0
              
            
            
              
                σ
                (
                X
                ,
                X
                )
              
              
                
                =
                
                  σ
                  
                    2
                  
                
                (
                X
                )
              
            
            
              
                σ
                (
                X
                ,
                Y
                )
              
              
                
                =
                σ
                (
                Y
                ,
                X
                )
              
            
            
              
                σ
                (
                a
                X
                ,
                b
                Y
                )
              
              
                
                =
                a
                b
                
                σ
                (
                X
                ,
                Y
                )
              
            
            
              
                σ
                (
                X
                +
                a
                ,
                Y
                +
                b
                )
              
              
                
                =
                σ
                (
                X
                ,
                Y
                )
              
            
            
              
                σ
                (
                a
                X
                +
                b
                Y
                ,
                c
                W
                +
                d
                V
                )
              
              
                
                =
                a
                c
                
                σ
                (
                X
                ,
                W
                )
                +
                a
                d
                
                σ
                (
                X
                ,
                V
                )
                +
                b
                c
                
                σ
                (
                Y
                ,
                W
                )
                +
                b
                d
                
                σ
                (
                Y
                ,
                V
                )
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}\sigma (X,a)&=0\\\sigma (X,X)&=\sigma ^{2}(X)\\\sigma (X,Y)&=\sigma (Y,X)\\\sigma (aX,bY)&=ab\,\sigma (X,Y)\\\sigma (X+a,Y+b)&=\sigma (X,Y)\\\sigma (aX+bY,cW+dV)&=ac\,\sigma (X,W)+ad\,\sigma (X,V)+bc\,\sigma (Y,W)+bd\,\sigma (Y,V)\end{aligned}}}
  Для послідовності випадкових змінних X1, …, Xn та сталих a1, …, an маємо
  
    
      
        
          σ
          
            2
          
        
        
          (
          
            
              ∑
              
                i
                =
                1
              
              
                n
              
            
            
              a
              
                i
              
            
            
              X
              
                i
              
            
          
          )
        
        =
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          a
          
            i
          
          
            2
          
        
        
          σ
          
            2
          
        
        (
        
          X
          
            i
          
        
        )
        +
        2
        
          ∑
          
            i
            ,
            j
            
            :
            
            i
            <
            j
          
        
        
          a
          
            i
          
        
        
          a
          
            j
          
        
        σ
        (
        
          X
          
            i
          
        
        ,
        
          X
          
            j
          
        
        )
        =
        
          ∑
          
            i
            ,
            j
          
        
        
          
            a
            
              i
            
          
          
            a
            
              j
            
          
          σ
          (
          
            X
            
              i
            
          
          ,
          
            X
            
              j
            
          
          )
        
      
    
    {\displaystyle \sigma ^{2}\left(\sum _{i=1}^{n}a_{i}X_{i}\right)=\sum _{i=1}^{n}a_{i}^{2}\sigma ^{2}(X_{i})+2\sum _{i,j\,:\,i<j}a_{i}a_{j}\sigma (X_{i},X_{j})=\sum _{i,j}{a_{i}a_{j}\sigma (X_{i},X_{j})}}
  Корисною тотожністю для обчислення коваріації між двома випадковими змінними 
  
    
      
        X
        ,
        Y
      
    
    {\displaystyle X,Y}
   є коваріаційна тотожність Хьофдинга (англ. Hoeffding's Covariance Identity):
  
    
      
        cov
        ⁡
        (
        X
        ,
        Y
        )
        =
        
          ∫
          
            
              R
            
          
        
        
          ∫
          
            
              R
            
          
        
        
          F
          
            (
            X
            ,
            Y
            )
          
        
        (
        x
        ,
        y
        )
        −
        
          F
          
            X
          
        
        (
        x
        )
        
          F
          
            Y
          
        
        (
        y
        )
        
        d
        x
        
        d
        y
      
    
    {\displaystyle \operatorname {cov} (X,Y)=\int _{\mathbb {R} }\int _{\mathbb {R} }F_{(X,Y)}(x,y)-F_{X}(x)F_{Y}(y)\,dx\,dy}
  де 
  
    
      
        
          F
          
            (
            X
            ,
            Y
            )
          
        
        (
        x
        ,
        y
        )
      
    
    {\displaystyle F_{(X,Y)}(x,y)}
   є функцією спільного розподілу випадкового вектора 
  
    
      
        (
        X
        ,
        Y
        )
      
    
    {\displaystyle (X,Y)}
  , а 
  
    
      
        
          F
          
            X
          
        
        (
        x
        )
        ,
        
          F
          
            Y
          
        
        (
        y
        )
      
    
    {\displaystyle F_{X}(x),F_{Y}(y)}
   є відособленими.


=== Загальніша тотожність для коваріаційних матриць ===
Нехай X буде випадковим вектором з коваріаційною матрицею Σ(X), і нехай A буде матрицею, яка може діяти на X. Коваріаційною матрицею матрично-векторного добутку A X є

  
    
      
        Σ
        (
        
          A
        
        
          X
        
        )
        =
        E
        ⁡
        [
        
          A
        
        
          X
        
        
          
            X
          
          
            
              T
            
          
        
        
          
            A
          
          
            
              T
            
          
        
        ]
        −
        E
        ⁡
        [
        
          A
        
        
          X
        
        ]
        E
        ⁡
        [
        
          
            X
          
          
            
              T
            
          
        
        
          
            A
          
          
            
              T
            
          
        
        ]
        =
        
          A
        
        Σ
        (
        
          X
        
        )
        
          
            A
          
          
            
              T
            
          
        
        .
      
    
    {\displaystyle \Sigma (\mathbf {A} \mathbf {X} )=\operatorname {E} [\mathbf {A} \mathbf {X} \mathbf {X} ^{\mathrm {T} }\mathbf {A} ^{\mathrm {T} }]-\operatorname {E} [\mathbf {A} \mathbf {X} ]\operatorname {E} [\mathbf {X} ^{\mathrm {T} }\mathbf {A} ^{\mathrm {T} }]=\mathbf {A} \Sigma (\mathbf {X} )\mathbf {A} ^{\mathrm {T} }.}
  Це є прямим результатом лінійності математичного сподівання, та є корисним при застосуванні до вектора лінійного перетворення, такого як перетворення вибілювання.


=== Некорельованість та незалежність ===
Якщо X та Y є незалежними, то їхня коваріація є нульовою. Це випливає з того, що за незалежності

  
    
      
        E
        ⁡
        [
        X
        Y
        ]
        =
        E
        ⁡
        [
        X
        ]
        ⋅
        E
        ⁡
        [
        Y
        ]
        .
      
    
    {\displaystyle \operatorname {E} [XY]=\operatorname {E} [X]\cdot \operatorname {E} [Y].}
  Обернене, проте, в загальному випадку не є вірним. Наприклад, нехай X буде рівномірно розподіленою на [−1, 1], і нехай Y = X2. Зрозуміло, що X та Y є залежними, але

  
    
      
        
          
            
              
                σ
                (
                X
                ,
                Y
                )
              
              
                
                =
                σ
                (
                X
                ,
                
                  X
                  
                    2
                  
                
                )
              
            
            
              
              
                
                =
                E
                ⁡
                [
                X
                ⋅
                
                  X
                  
                    2
                  
                
                ]
                −
                E
                ⁡
                [
                X
                ]
                ⋅
                E
                ⁡
                [
                
                  X
                  
                    2
                  
                
                ]
              
            
            
              
              
                
                =
                E
                
                
                  [
                  
                    X
                    
                      3
                    
                  
                  ]
                
                −
                E
                ⁡
                [
                X
                ]
                E
                ⁡
                [
                
                  X
                  
                    2
                  
                
                ]
              
            
            
              
              
                
                =
                0
                −
                0
                ⋅
                E
                ⁡
                [
                
                  X
                  
                    2
                  
                
                ]
              
            
            
              
              
                
                =
                0.
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}\sigma (X,Y)&=\sigma (X,X^{2})\\&=\operatorname {E} [X\cdot X^{2}]-\operatorname {E} [X]\cdot \operatorname {E} [X^{2}]\\&=\operatorname {E} \!\left[X^{3}\right]-\operatorname {E} [X]\operatorname {E} [X^{2}]\\&=0-0\cdot \operatorname {E} [X^{2}]\\&=0.\end{aligned}}}
  В цьому випадку взаємозв'язок між Y та X є нелінійним, тоді як кореляція та коваріація є мірами лінійної залежності між двома змінними. Цей приклад показує, що, якщо дві змінні є некорельованими, це в загальному випадку не означає, що вони є незалежними. Проте, якщо дві змінні є спільно нормально розподіленими (але не якщо вони є просто відособлено нормально розподіленими), то некорельованість дійсно означає незалежність.


=== Зв'язок із внутрішніми добутками ===
Багато властивостей коваріації можна елегантно здобути, звернувши увагу на те, що вона задовольняє властивості, подібні до властивостей внутрішнього добутку:

білінійність: для сталих a та b, та випадкових змінних X, Y, Z, σ(aX + bY, Z) = a σ(X, Z) + b σ(Y, Z);
симетричність: σ(X, Y) = σ(Y, X);
додатна напівозначеність: σ2(X) = σ(X, X) ≥ 0 для всіх випадкових змінних X, а σ(X, X) = 0 означає, що X є сталою випадковою змінною (K).Насправді ці властивості означають, що коваріація визначає внутрішній добуток над векторним фактор-простором, отримуваним взяттям підпростору випадкових змінних зі скінченним другим моментом, та визначенням будь-яких двох, які відрізняються на сталу. (Це визначення перетворює згадану вище додатну напівозначеність на додатноозначеність.) Цей векторний фактор-простір є ізоморфним до підпростору випадкових змінних зі скінченним другим моментом та нульовим середнім значенням; на цьому підпросторі коваріація в точності дорівнює внутрішньому добуткові L2 дійснозначних функцій на вибірковому просторі.
В результаті, для випадкових змінних зі скінченною дисперсією нерівність

  
    
      
        
          |
        
        σ
        (
        X
        ,
        Y
        )
        
          |
        
        ≤
        
          
            
              σ
              
                2
              
            
            (
            X
            )
            
              σ
              
                2
              
            
            (
            Y
            )
          
        
      
    
    {\displaystyle |\sigma (X,Y)|\leq {\sqrt {\sigma ^{2}(X)\sigma ^{2}(Y)}}}
  виконується через нерівність Коші — Буняковського.
Доведення: Якщо σ2(Y) = 0, то вона виконується тривіально. Інакше, нехай випадкова змінна

  
    
      
        Z
        =
        X
        −
        
          
            
              σ
              (
              X
              ,
              Y
              )
            
            
              
                σ
                
                  2
                
              
              (
              Y
              )
            
          
        
        Y
        .
      
    
    {\displaystyle Z=X-{\frac {\sigma (X,Y)}{\sigma ^{2}(Y)}}Y.}
  Тоді ми маємо

  
    
      
        
          
            
              
                0
                ≤
                
                  σ
                  
                    2
                  
                
                (
                Z
                )
              
              
                
                =
                σ
                
                  (
                  
                    X
                    −
                    
                      
                        
                          σ
                          (
                          X
                          ,
                          Y
                          )
                        
                        
                          
                            σ
                            
                              2
                            
                          
                          (
                          Y
                          )
                        
                      
                    
                    Y
                    ,
                    X
                    −
                    
                      
                        
                          σ
                          (
                          X
                          ,
                          Y
                          )
                        
                        
                          
                            σ
                            
                              2
                            
                          
                          (
                          Y
                          )
                        
                      
                    
                    Y
                  
                  )
                
              
            
            
              
              
                
                =
                
                  σ
                  
                    2
                  
                
                (
                X
                )
                −
                
                  
                    
                      (
                      σ
                      (
                      X
                      ,
                      Y
                      )
                      
                        )
                        
                          2
                        
                      
                    
                    
                      
                        σ
                        
                          2
                        
                      
                      (
                      Y
                      )
                    
                  
                
                .
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}0\leq \sigma ^{2}(Z)&=\sigma \left(X-{\frac {\sigma (X,Y)}{\sigma ^{2}(Y)}}Y,X-{\frac {\sigma (X,Y)}{\sigma ^{2}(Y)}}Y\right)\\[12pt]&=\sigma ^{2}(X)-{\frac {(\sigma (X,Y))^{2}}{\sigma ^{2}(Y)}}.\end{aligned}}}
  


== Обчислення вибіркової коваріації ==

Вибіркова коваріація N спостережень K змінних — це матриця 
  
    
      
        
          
            
              
                q
                ¯
              
              ¯
            
          
          =
          
            [
            
              [
              
                q
                
                  j
                  k
                
              
              ]
            
            ]
          
        
      
    
    {\displaystyle \textstyle {\overline {\overline {q}}}=\left[[q_{jk}]\right]}
   розміру K-на-K з елементами

  
    
      
        
          q
          
            j
            k
          
        
        =
        
          
            1
            
              N
              −
              1
            
          
        
        
          ∑
          
            i
            =
            1
          
          
            N
          
        
        
          (
          
            
              X
              
                i
                j
              
            
            −
            
              
                
                  
                    X
                    ¯
                  
                
              
              
                j
              
            
          
          )
        
        
          (
          
            
              X
              
                i
                k
              
            
            −
            
              
                
                  
                    X
                    ¯
                  
                
              
              
                k
              
            
          
          )
        
        ,
      
    
    {\displaystyle q_{jk}={\frac {1}{N-1}}\sum _{i=1}^{N}\left(X_{ij}-{\bar {X}}_{j}\right)\left(X_{ik}-{\bar {X}}_{k}\right),}
  що є оцінкою коваріації між змінною j та змінною k.
Матриці вибіркового середнього та вибіркової коваріації є незміщеними оцінками середнього значення та коваріаційної матриці випадкового вектора 
  
    
      
        
          
            X
          
        
      
    
    {\displaystyle \textstyle \mathbf {X} }
  , рядкового вектора, чий j-тий елемент (j = 1, …, K) є однією з випадкових змінних. Причиною того, що коваріаційна матриця має в знаменнику 
  
    
      
        
          N
          −
          1
        
      
    
    {\displaystyle \textstyle N-1}
  , а не 
  
    
      
        
          N
        
      
    
    {\displaystyle \textstyle N}
  , по суті є те, що середнє значення сукупності 
  
    
      
        E
        ⁡
        (
        X
        )
      
    
    {\displaystyle \operatorname {E} (X)}
   не відоме, і замінене вибірковим середнім значенням 
  
    
      
        
          
            
              X
              ¯
            
          
        
      
    
    {\displaystyle \mathbf {\bar {X}} }
  . Якщо середнє значення сукупності 
  
    
      
        E
        ⁡
        (
        X
        )
      
    
    {\displaystyle \operatorname {E} (X)}
   є відомим, то аналогічна незміщена оцінка задається як

  
    
      
        
          q
          
            j
            k
          
        
        =
        
          
            1
            N
          
        
        
          ∑
          
            i
            =
            1
          
          
            N
          
        
        
          (
          
            
              X
              
                i
                j
              
            
            −
            E
            ⁡
            (
            
              X
              
                j
              
            
            )
          
          )
        
        
          (
          
            
              X
              
                i
                k
              
            
            −
            E
            ⁡
            (
            
              X
              
                k
              
            
            )
          
          )
        
        .
      
    
    {\displaystyle q_{jk}={\frac {1}{N}}\sum _{i=1}^{N}\left(X_{ij}-\operatorname {E} (X_{j})\right)\left(X_{ik}-\operatorname {E} (X_{k})\right).}
  


== Коментарі ==
Коваріацію іноді називають мірою «лінійної залежності» між двома випадковими змінними. Це не означає те ж саме, що й у контексті лінійної алгебри (див. лінійну залежність). Коли коваріацію унормовано, отримують коефіцієнт кореляції. З нього можливо отримати коефіцієнт Пірсона, який дає допасованість для найкращої з можливих лінійних функцій, що описують взаємозв'язок між змінними. В цьому сенсі коваріація є лінійним мірилом залежності.


== Застосування ==


=== В генетиці та молекулярній біології ===
Коваріація є важливою мірою в біології. Деякі послідовності ДНК є консервативнішими за інші серед різних видів, і відтак для дослідження вторинних та третинних структур білків, або структур РНК, порівнюють послідовності близько споріднених видів. Якщо знайдено зміни послідовностей, або взагалі не знайдено змін у некодувальній РНК (такій як мікроРНК), то послідовності вважають потрібними для загальних структурних лейтмотивів, таких як цикл РНК.


=== У фінансовій економіці ===
Коваріації відіграють важливу роль у фінансовій економіці, особливо в портфельному аналізі та в моделі ціноутворення капітальних активів. Коваріації серед виручок різних активів використовують для визначення, за деяких припущень, відносних сум різних активів, які інвестор повинен (в нормативному аналізі) або, як передбачається, буде (в позитивному аналізі) обирати для тримання в контексті диверсифікації.


=== В опрацьовуванні метеорологічних та океанографічних даних ===
Коваріаційна матриця є важливою в оцінюванні початкових умов, необхідних для запуску моделей прогнозу погоди. «Коваріаційну матрицю похибки прогнозу» (англ. forecast error covariance matrix) зазвичай будують між збуреннями навколо середнього стану (чи то кліматологічного, чи то ансамблевого середнього). «Коваріаційну матрицю похибки спостереження» (англ. observation error covariance matrix) будують для представлення величини об'єднаних похибок спостереження (на діагоналі) та корельованих похибок між вимірюваннями (поза діагоналлю).


=== У виділянні ознак ===
Коваріаційну матрицю застосовують для збирання даних про спектральну мінливість сигналу.


== Див. також ==


== Примітки ==


== Література ==
Пряха Б. Про зв'язок дисперсій та коваріацій // Геодезія, картографія і аерофотознімання, Львів: Видавництво Національного університету «Львівська політехніка». — 2009. — Вип. 71. — С. 262—271.


== Посилання ==
Hazewinkel, Michiel, ред. (2001). Covariance. Encyclopedia of Mathematics. Springer. ISBN 978-1-55608-010-4.  (англ.)
Сторінка MathWorld про обчислення вибіркової коваріації (англ.)
Навчальний посібник з коваріації із застосуванням R (англ.)
Коваріація та кореляція (англ.)